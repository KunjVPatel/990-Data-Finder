{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Calls to ProPublica for Retrieving Filing Data\n",
    "\n",
    "This outlines the process of making API calls to ProPublica to obtain additional data. The purpose of these API calls is to get the Names, Previous Years Revenue for reference and NTEE code to categorize based on which non profit.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "The motivation behind making API calls to ProPublica is twofold:\n",
    "\n",
    "1. **Initial Understanding:** These calls provide an initial understanding of the data required for the project.\n",
    "\n",
    "2. **Additional Features:** ProPublica's API offers some features and data that can enhance our main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries for the API Calls projects\n",
    "import os # For pathing and batch folder naming\n",
    "import requests # This library is the reson why API calls work\n",
    "import pandas as pd # Good ol` pandas, cant do any data scraping without this puppy\n",
    "import concurrent.futures # Did someone say multithreading?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Function to get our Data and Write to a CSV\n",
    "\n",
    "1. **API Endpoint:** We accessed ProPublica's API, then we queried the data based on Employer Identification Numbers (EINs). Since ProPublica for nonprofits is free we just get the data.\n",
    "\n",
    "2. **Data Collection:** The API calls fetched data for organizations based on their EINs. This data includes financial information, tax period years, PDF document URLs, organization details (such as name, city, state), and more.\n",
    "\n",
    "3. **Multithreading:** To optimize the data retrieval process, we used multithreading using the `concurrent.futures.ThreadPoolExecutor` to parallelize the API requests and to learn more about multi threading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def einLookup(eins): # Our einLookup function, that take EIN and puts the data in the result \n",
    "    results = []\n",
    "\n",
    "    def fetch_data(ein): # Fetching the Data, essentially sending query and getting output\n",
    "        try: # Try catch, good practice to prevent disruptions\n",
    "            q = f'https://projects.propublica.org/nonprofits/api/v2/organizations/{ein}.json'\n",
    "            response = requests.get(q) \n",
    "            response.raise_for_status()\n",
    "            data = response.json() # Getting our data from the response of the API\n",
    "            # print(data)\n",
    "\n",
    "            if 'filings_with_data' in data and data['filings_with_data']: # Now we look for filings with data in the data given\n",
    "                filingData = data['filings_with_data'][0] # So if we find it (since filings will contain our total rev and total expenses)\n",
    "                yearEnd = filingData.get('tax_prd_yr', 'N/A') # Get the year\n",
    "\n",
    "                totRevenue = filingData.get('totrevenue', 'N/A') # Total rev\n",
    "                totfuncexpns = filingData.get('totfuncexpns', 'N/A') # Total Expenses\n",
    "                netRevenue = totRevenue - totfuncexpns # Calculating net rev\n",
    "\n",
    "                result = [ # Storing our result here to later append to the csv\n",
    "                    ein,\n",
    "                    data['organization']['name'],\n",
    "                    totRevenue,\n",
    "                    netRevenue,\n",
    "                    yearEnd,\n",
    "                    filingData.get('pdf_url', 'N/A'),\n",
    "                    data['organization'].get('city', 'N/A'),\n",
    "                    data['organization'].get('state', 'N/A'),\n",
    "                    data['organization'].get('ntee_code', 'N/A'),\n",
    "                    filingData.get('formtype', 'N/A')\n",
    "                ]\n",
    "                return result # Returning results\n",
    "            else:\n",
    "                print(f'No data found for EIN {ein}') # If no data is found, just mention EIN of skipped\n",
    "                return None\n",
    "        except Exception as e: # If there is exception we catch it\n",
    "            print(f'Error for EIN {ein}: {e}')\n",
    "            return None\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor: # Multithreading it for 8 cores\n",
    "        results = list(executor.map(fetch_data, eins))\n",
    "\n",
    "    results = [r for r in results if r is not None]\n",
    "\n",
    "    return results\n",
    "\n",
    "def csvWriter(filename, data): # The infamous CSV writer\n",
    "    # Setting Col names\n",
    "    df = pd.DataFrame(data, columns=['EIN_value', 'Name', 'Gross_Revenue', 'Net_Revenue', 'Tax_Period_Year', 'PDF_URL', 'City', 'State', 'NTEE_Code', 'Form_Type'])\n",
    "    df.to_csv(filename, index=False) # Converting to csv\n",
    "    print(f'Data has been written to {filename}.') # Writing to file\n",
    "\n",
    "def einLoader(): # Now our EIN value loader, and where we get the data\n",
    "    # Setting path to read data, more specifically EIN values from the XML data set pulled from the IRS\n",
    "    einPath = \"../OpportunityHack/Data/einval.xlsx\"\n",
    "    einVal = pd.read_excel(einPath)\n",
    "    einList = [str(ein) for ein in einVal['EIN']]\n",
    "\n",
    "    # Setting path to store the batches of data\n",
    "    batchFolder = \"../OpportunityHack/Batch\"\n",
    "    batchSize = 50 # Batches are nice, since you can see data is loading, easy to merge and you get some data\n",
    "    length = len(einList)\n",
    "    # length = 2 # Testing for single batch to see what we get\n",
    "\n",
    "    for i in range(0, length, batchSize): # Loop through length, write files to each batch and store them\n",
    "        batch = einList[i:i + batchSize]\n",
    "        results = einLookup(batch)\n",
    "        batchFileName = os.path.join(batchFolder, f'out_batch_{i // batchSize}.csv')\n",
    "        csvWriter(batchFileName, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling our Main Function for actually getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\": # The Main Function where we call our function to get the data\n",
    "    einLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Merger that Merges the Batches CSV to a final data set for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathOfBatch = \"../OpportunityHack/Batch\" # We read the dir where we have batch\n",
    "filePath = os.listdir(pathOfBatch) # set file path to os.listdir\n",
    "files = [file for file in filePath if os.path.isfile(os.path.join(pathOfBatch, file))] # Add each path\n",
    "\n",
    "num = len(files) + 1 # Set num to the count of files + 1 to loop through, merge and delete them\n",
    "mergeDF = pd.DataFrame() # Making the mergeDF an actual df\n",
    "\n",
    "for i in range(num): # Looping through num of files\n",
    "    filename = f'../OpportunityHack/Batch/out_batch_{i}.csv' \n",
    "    df = pd.read_csv(filename) # Reading each file\n",
    "    mergeDF = pd.concat([mergeDF, df], ignore_index=True) # Concatinating them\n",
    "\n",
    "mergeName = '../OpportunityHack/Data/merged_out.csv' # Setting our merged csv\n",
    "mergeDF.to_csv(mergeName, index=False)\n",
    "\n",
    "for i in range(num): # Now removing the renaming batch files\n",
    "    filename = f'../OpportunityHack/Batch/out_batch_{i}.csv'\n",
    "    os.remove(filename)\n",
    "\n",
    "print(f'Merged data successfully written to {mergeName}. Batch files have been deleted.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
